{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T20:54:57.903713Z",
     "start_time": "2025-04-23T20:54:57.891840Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, defaultdict, deque\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "Point = namedtuple('Point', ['x', 'y'])\n",
    "def distance(p1: Point, p2: Point):\n",
    "    return (p1[0] - p2[0])**2 + (p1[1] - p2[1])**2\n",
    "\n",
    "class IsoscelesTriangle:\n",
    "    def __init__(self, m:int, n:int):\n",
    "        self.grid_size = (m,n)\n",
    "        self.action_space = [Point(x,y) for x in range(n) for y in range(m)]\n",
    "        self.points = []\n",
    "        self.distance_map = {}\n",
    "\n",
    "    def add_point(self, point: Point):\n",
    "        '''\n",
    "        :param point:\n",
    "        :return: True/False: either find an isosceles triangle, and the game stops\n",
    "                 tuple of 3 points/None: the isosceles_triangle found, or else None\n",
    "                 the updated state of the game\n",
    "        '''\n",
    "        # Check if the input is valid\n",
    "        if not self._valid_action(point):\n",
    "            # print(f\"Invalid Point {point[0]}, {point[1]} Added\")\n",
    "            return False, None, self.points, -10\n",
    "\n",
    "        self.distance_map[point] = defaultdict(list)\n",
    "        for p in self.points:\n",
    "            d = distance(point, p)\n",
    "            self.distance_map[p][d].append(point)\n",
    "            self.distance_map[point][d].append(p)\n",
    "            if len(self.distance_map[point][d]) >= 2:\n",
    "                return True, (point, p, self.distance_map[point][d][0]), self.points, 0\n",
    "            if len(self.distance_map[p][d]) >= 2:\n",
    "                return True, (point, p, self.distance_map[p][d][0]), self.points, 0\n",
    "\n",
    "        self.action_space.remove(point)\n",
    "        self.points.append(point)\n",
    "        return False, None, self.points, 1\n",
    "\n",
    "    def _valid_action(self, point: Point):\n",
    "        if point not in self.points and 0 <= point[0] < self.grid_size[1] and 0 <= point[1] < self.grid_size[0]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def decode_action(self, action: int):\n",
    "        m,n = self.grid_size\n",
    "        x,y= divmod(action,n)\n",
    "        return Point(x,y)\n",
    "\n",
    "    def encode_action(self, point: Point):\n",
    "        m,n = self.grid_size\n",
    "        return point[0] * n + point[1]\n",
    "\n",
    "    # this will return a list of points that we can add\n",
    "    def get_action_space(self):\n",
    "        return self.action_space\n",
    "\n",
    "    # each point can exist or not exist.\n",
    "    def get_state_space_size(self):\n",
    "        return self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "    def reset(self):\n",
    "        # This will reset the game, and return the current states\n",
    "        del self.points\n",
    "        del self.distance_map\n",
    "        del self.action_space\n",
    "        self.points = []\n",
    "        self.distance_map = {}\n",
    "        self.action_space = [Point(x,y) for x in range(self.grid_size[1]) for y in range(self.grid_size[0])]\n",
    "        return self.points\n",
    "\n",
    "    def encode_state(self, points):\n",
    "        \"\"\"\n",
    "        Encodes the selected points into a binary vector of length grid_size * grid_size\n",
    "        \"\"\"\n",
    "        m,n = self.grid_size\n",
    "        state = torch.zeros(m * n, dtype=torch.float32)\n",
    "        for x, y in points:\n",
    "            index = x * n + y  # row-major order\n",
    "            state[index] = 1.0\n",
    "        return state\n",
    "\n",
    "# Define the neural network for Q-learning\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Hyperparameters for models\n",
    "gamma = 1\n",
    "epsilon = 0.05\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "target_update_freq = 10\n",
    "memory_size = 10000\n",
    "episodes = 50000\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:55:18.423024Z",
     "start_time": "2025-04-23T20:55:18.363451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# m is number of rows, and n is number of cols\n",
    "m = 3\n",
    "n = 6\n",
    "env = IsoscelesTriangle(m, n)\n",
    "input_dim = m * n\n",
    "output_dim = m * n\n",
    "\n",
    "policy_net = DQN(input_dim, output_dim)\n",
    "target_net = DQN(input_dim, output_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.encode_state(env.reset())\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # epsilon greedy algorithm\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        if random.random() < epsilon:\n",
    "            action = random.sample(env.action_space, 1)[0]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state_tensor).argmax().item()\n",
    "                action = env.decode_action(action)\n",
    "\n",
    "        terminated, result_found, next_state_raw, reward = env.add_point(action)\n",
    "        action = env.encode_action(action)\n",
    "        next_state = env.encode_state(next_state_raw)\n",
    "        done = terminated\n",
    "\n",
    "        if not done:\n",
    "            total_reward += reward\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "        # Training step\n",
    "        # If we get enough data sampled from the action of gradient, then we train our network\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            # states = torch.tensor(states, dtype=torch.float32)\n",
    "            states = torch.stack(states).float()\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "            # next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            next_states = torch.stack(next_states).float()\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "            next_q_values = target_net(next_states).max(1, keepdim=True)[0]\n",
    "            targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            loss = nn.functional.mse_loss(q_values, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    # Update target network\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    if episode % 500 == 0:\n",
    "        print(f\"at episode {episode}, total reward {total_reward} with fulllist of points: {next_state_raw}\")\n",
    "\n",
    "# env.close()\n",
    "\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(10)/10, mode='valid'))\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (window=10)\")\n",
    "plt.title(\"DQN on Triangle\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "f551149143a12269",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianhao\\AppData\\Local\\Temp\\ipykernel_21508\\53050548.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 24 is out of bounds for dimension 0 with size 18",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 36\u001B[39m\n\u001B[32m     34\u001B[39m terminated, result_found, next_state_raw, reward = env.add_point(action)\n\u001B[32m     35\u001B[39m action = env.encode_action(action)\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m next_state = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state_raw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     37\u001B[39m done = terminated\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 88\u001B[39m, in \u001B[36mIsoscelesTriangle.encode_state\u001B[39m\u001B[34m(self, points)\u001B[39m\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m x, y \u001B[38;5;129;01min\u001B[39;00m points:\n\u001B[32m     87\u001B[39m     index = x * n + y  \u001B[38;5;66;03m# row-major order\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m88\u001B[39m     \u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m = \u001B[32m1.0\u001B[39m\n\u001B[32m     89\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m state\n",
      "\u001B[31mIndexError\u001B[39m: index 24 is out of bounds for dimension 0 with size 18"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:52:03.452814Z",
     "start_time": "2025-04-23T20:52:03.442889Z"
    }
   },
   "cell_type": "code",
   "source": "max(episode_rewards)",
   "id": "19e64068d35ffad6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:52:21.118011Z",
     "start_time": "2025-04-23T20:52:21.102383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state = env.encode_state(env.reset())\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        action = policy_net(state_tensor).argmax().item()\n",
    "        action = env.decode_action(action)\n",
    "\n",
    "    terminated, result_found, next_state_raw, reward = env.add_point(action)\n",
    "    action = env.encode_action(action)\n",
    "    next_state = env.encode_state(next_state_raw)\n",
    "    done = terminated\n",
    "\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "\n",
    "print(total_reward)"
   ],
   "id": "cf536fc40a0cc59c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianhao\\AppData\\Local\\Temp\\ipykernel_21508\\1080033082.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TODO, play the game in perfection, and return the number of points\n",
    "\n",
    "there is actually lower bound on how much we can get"
   ],
   "id": "1b4ed783ff3828da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fe3048dddb34b788"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
